# app.py (vers√£o com Melhoria 1 e 2)

import streamlit as st
import json
import numpy as np
import pandas as pd
import faiss
from sentence_transformers import SentenceTransformer, CrossEncoder
import requests
import re
import unicodedata
import logging
from pathlib import Path
import zipfile
import io
import shutil
from concurrent.futures import ThreadPoolExecutor # <<< MELHORIA 4 ADICIONADA
from tools import (
    find_companies_by_topic,
    get_final_unified_answer,
    suggest_alternative_query,
    analyze_topic_thematically, 
    get_summary_for_topic_at_company,
    rerank_with_cross_encoder,
    _create_alias_to_canonical_map, 
    _get_all_canonical_topics_from_text # <-- CORRE√á√ÉO: Import adicionado
)
logger = logging.getLogger(__name__)

# --- M√≥dulos do Projeto (devem estar na mesma pasta) ---
from knowledge_base import DICIONARIO_UNIFICADO_HIERARQUICO
from analytical_engine import AnalyticalEngine

# --- Configura√ß√µes Gerais ---
st.set_page_config(page_title="Agente de An√°lise LTIP", page_icon="üîç", layout="wide", initial_sidebar_state="expanded")

MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
TOP_K_SEARCH = 7
GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY", "")
GEMINI_MODEL = "gemini-2.0-flash-lite"
CVM_SEARCH_URL = "https://www.rad.cvm.gov.br/ENET/frmConsultaExternaCVM.aspx"

FILES_TO_DOWNLOAD = {
    "item_8_4_chunks_map_final.json": "https://github.com/tovarich86/agentev2/releases/download/V1.0-data/item_8_4_chunks_map_final.json",
    "item_8_4_faiss_index_final.bin": "https://github.com/tovarich86/agentev2/releases/download/V1.0-data/item_8_4_faiss_index_final.bin",
    "outros_documentos_chunks_map_final.json": "https://github.com/tovarich86/agentev2/releases/download/V1.0-data/outros_documentos_chunks_map_final.json",
    "outros_documentos_faiss_index_final.bin": "https://github.com/tovarich86/agentev2/releases/download/V1.0-data/outros_documentos_faiss_index_final.bin",
    "resumo_fatos_e_topicos_final_enriquecido.json": "https://github.com/tovarich86/agentev2/releases/download/V1.0-data/resumo_fatos_e_topicos_final_enriquecido.json"
}
CACHE_DIR = Path("data_cache")
SUMMARY_FILENAME = "resumo_fatos_e_topicos_final_enriquecido.json"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- CARREGADOR DE DADOS ---
@st.cache_resource(show_spinner="Configurando o ambiente e baixando dados...")
def setup_and_load_data():
    CACHE_DIR.mkdir(exist_ok=True)
    
    for filename, url in FILES_TO_DOWNLOAD.items():
        local_path = CACHE_DIR / filename
        if not local_path.exists():
            logger.info(f"Baixando arquivo '{filename}'...")
            try:
                response = requests.get(url, stream=True, timeout=60)
                response.raise_for_status()
                with open(local_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                logger.info(f"'{filename}' baixado com sucesso.")
            except requests.exceptions.RequestException as e:
                st.error(f"Erro ao baixar {filename} de {url}: {e}")
                st.stop()
    # --- Carregamento de Modelos ---
    st.write("Carregando modelo de embedding...")
    embedding_model = SentenceTransformer(MODEL_NAME)
    
    st.write("Carregando modelo de Re-ranking (Cross-Encoder)...")
    cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    artifacts = {}
    for index_file in CACHE_DIR.glob('*_faiss_index_final.bin'):
        category = index_file.stem.replace('_faiss_index_final', '')
        chunks_file = CACHE_DIR / f"{category}_chunks_map_final.json"
        try:
            artifacts[category] = {
                'index': faiss.read_index(str(index_file)),
                'chunks': json.load(open(chunks_file, 'r', encoding='utf-8'))
            }
        except Exception as e:
            st.error(f"Falha ao carregar artefatos para a categoria '{category}': {e}")
            st.stop()

    summary_file_path = CACHE_DIR / SUMMARY_FILENAME
    try:
        with open(summary_file_path, 'r', encoding='utf-8') as f:
            summary_data = json.load(f)
    except FileNotFoundError:
        st.error(f"Erro cr√≠tico: '{SUMMARY_FILENAME}' n√£o foi encontrado.")
        st.stop()
        
    return embedding_model, cross_encoder_model, artifacts, summary_data



# --- FUN√á√ïES GLOBAIS E DE RAG ---

def _create_flat_alias_map(kb: dict) -> dict:
    alias_to_canonical = {}
    for section, topics in kb.items():
        for topic_name_raw, aliases in topics.items():
            canonical_name = topic_name_raw.replace('_', ' ')
            alias_to_canonical[canonical_name.lower()] = canonical_name
            for alias in aliases:
                alias_to_canonical[alias.lower()] = canonical_name
    return alias_to_canonical

AVAILABLE_TOPICS = list(set(_create_flat_alias_map(DICIONARIO_UNIFICADO_HIERARQUICO).values()))

def expand_search_terms(base_term: str, kb: dict) -> list[str]:
    base_term_lower = base_term.lower()
    expanded_terms = {base_term_lower}
    for section, topics in kb.items():
        for topic, aliases in topics.items():
            all_terms_in_group = {alias.lower() for alias in aliases} | {topic.lower().replace('_', ' ')}
            if base_term_lower in all_terms_in_group:
                expanded_terms.update(all_terms_in_group)
    return list(expanded_terms)

# Em app.py, substitua esta fun√ß√£o
def search_by_tags(artifacts: dict, company_name: str, target_tags: list) -> list:
    results = []
    searchable_company_name = unicodedata.normalize('NFKD', company_name.lower()).encode('ascii', 'ignore').decode('utf-8').split(' ')[0]
    target_tags_lower = {tag.lower() for tag in target_tags}
    
    for index_name, artifact_data in artifacts.items():
        chunk_map = artifact_data.get('chunks', {}).get('map', [])
        all_chunks_text = artifact_data.get('chunks', {}).get('chunks', [])
        for i, mapping in enumerate(chunk_map):
            if searchable_company_name in mapping.get("company_name", "").lower():
                chunk_text = all_chunks_text[i]
                found_topics_in_chunk = re.findall(r'\[topico:([^\]]+)\]', chunk_text)
                if found_topics_in_chunk:
                    topics_in_chunk_set = {t.lower() for t in found_topics_in_chunk[0].split(',')}
                    intersection = target_tags_lower.intersection(topics_in_chunk_set)
                    if intersection:
                        # --- CORRE√á√ÉO APLICADA AQUI ---
                        # Padroniza o dicion√°rio de sa√≠da para usar as chaves corretas
                        result_item = {
                            "text": all_chunks_text[i],
                            "company_name": mapping.get("company_name"),
                            "source_url": mapping.get("source_url", "N/A"),
                            # Mant√©m informa√ß√µes √∫teis para a fun√ß√£o que a chama
                            "doc_type": index_name 
                        }
                        results.append(result_item)
    return results

def get_final_unified_answer(query: str, context: str) -> str:
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent?key={GEMINI_API_KEY}"
    has_complete_8_4 = "formul√°rio de refer√™ncia" in query.lower() and "8.4" in query.lower()
    has_tagged_chunks = "--- CONTE√öDO RELEVANTE" in context
    structure_instruction = "Organize a resposta de forma l√≥gica e clara usando Markdown."
    if has_complete_8_4:
        structure_instruction = "ESTRUTURA OBRIGAT√ìRIA PARA ITEM 8.4: Use a estrutura oficial do item 8.4 do Formul√°rio de Refer√™ncia (a, b, c...)."
    elif has_tagged_chunks:
        structure_instruction = "PRIORIZE as informa√ß√µes dos chunks recuperados e organize a resposta de forma l√≥gica."
    prompt = f"""Voc√™ √© um consultor especialista em planos de incentivo de longo prazo (ILP).
    PERGUNTA ORIGINAL DO USU√ÅRIO: "{query}"
    CONTEXTO COLETADO DOS DOCUMENTOS:
    {context}
    {structure_instruction}
    INSTRU√á√ïES PARA O RELAT√ìRIO FINAL:
    1. Responda diretamente √† pergunta do usu√°rio com base no contexto fornecido.
    2. Seja detalhado, preciso e profissional na sua linguagem. Use formata√ß√£o Markdown.
    3. Se uma informa√ß√£o espec√≠fica pedida n√£o estiver no contexto, declare explicitamente: "Informa√ß√£o n√£o encontrada nas fontes analisadas.". N√£o invente dados.
    RELAT√ìRIO ANAL√çTICO FINAL:"""
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    headers = {'Content-Type': 'application/json'}
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=180)
        response.raise_for_status()
        return response.json()['candidates'][0]['content']['parts'][0]['text'].strip()
    except Exception as e:
        logger.error(f"ERRO ao gerar resposta final com LLM: {e}")
        return f"Ocorreu um erro ao contatar o modelo de linguagem. Detalhes: {str(e)}"

# <<< MELHORIA 1 ADICIONADA >>>
def get_query_intent_with_llm(query: str) -> str:
    """
    Usa um LLM para classificar a inten√ß√£o do usu√°rio em 'quantitativa' ou 'qualitativa'.
    Retorna 'qualitativa' como padr√£o em caso de erro.
    """
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent?key={GEMINI_API_KEY}"
    
    prompt = f"""
    Analise a pergunta do usu√°rio e classifique a sua inten√ß√£o principal. Responda APENAS com uma √∫nica palavra em JSON.
    
    As op√ß√µes de classifica√ß√£o s√£o:
    1. "quantitativa": Se a pergunta busca por n√∫meros, listas diretas, contagens, m√©dias, estat√≠sticas ou agrega√ß√µes. 
       Exemplos: "Quantas empresas t√™m TSR Relativo?", "Qual a m√©dia de vesting?", "Liste as empresas com desconto no strike.".
    2. "qualitativa": Se a pergunta busca por explica√ß√µes, detalhes, compara√ß√µes, descri√ß√µes ou an√°lises aprofundadas.
       Exemplos: "Como funciona o plano da Vale?", "Compare os planos da Hypera e Movida.", "Detalhe o tratamento de dividendos.".

    Pergunta do Usu√°rio: "{query}"

    Responda apenas com o JSON da classifica√ß√£o. Exemplo de resposta: {{"intent": "qualitativa"}}
    """
    
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {
            "temperature": 0.0,
            "maxOutputTokens": 50
        }
    }
    headers = {'Content-Type': 'application/json'}

    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=30)
        response.raise_for_status()
        
        response_text = response.json()['candidates'][0]['content']['parts'][0]['text']
        intent_json = json.loads(re.search(r'\{.*\}', response_text, re.DOTALL).group())
        intent = intent_json.get("intent", "qualitativa").lower()
        
        logger.info(f"Inten√ß√£o detectada pelo LLM: '{intent}' para a pergunta: '{query}'")
        
        if intent in ["quantitativa", "qualitativa"]:
            return intent
        else:
            logger.warning(f"Inten√ß√£o n√£o reconhecida '{intent}'. Usando 'qualitativa' como padr√£o.")
            return "qualitativa"

    except Exception as e:
        logger.error(f"ERRO ao determinar inten√ß√£o com LLM: {e}. Usando 'qualitativa' como padr√£o.")
        return "qualitativa"

# <<< MELHORIA 2 APLICADA >>>
# Fun√ß√£o modificada para lidar com buscas gerais (sem empresa)
# Em app.py, substitua esta fun√ß√£o
def execute_dynamic_plan(
    query: str,
    plan: dict,
    artifacts: dict,
    model: SentenceTransformer,
    cross_encoder_model: CrossEncoder,
    kb: dict
) -> tuple[str, list[dict]]:
    """
    Vers√£o definitiva e completa do Executor de Planos.
    Esta fun√ß√£o central analisa o 'plano' e seleciona a melhor estrat√©gia de busca
    de documentos, utilizando um nome de busca limpo para a busca vetorial e uma
    l√≥gica de verifica√ß√£o h√≠brida (cat√°logo + normaliza√ß√£o) para a valida√ß√£o final.
    Todos os resultados s√£o ent√£o re-ranqueados para m√°xima relev√¢ncia.
    """
    # Carrega recursos necess√°rios do estado da sess√£o do Streamlit
    company_catalog_rich = st.session_state.get("company_catalog_rich", [])
    company_lookup_map = st.session_state.get("company_lookup_map", {})
    if not company_lookup_map:
        logger.error("O mapa de consulta de empresas (company_lookup_map) n√£o foi encontrado no st.session_state.")
        return "Erro de configura√ß√£o interna: o mapa de empresas n√£o foi carregado.", []

    candidate_chunks = {}
    TOP_K_INITIAL_RETRIEVAL = 30
    TOP_K_SEARCH_FINAL = 15

    plan_type = plan.get("plan_type", "default")
    empresas = plan.get("empresas", [])
    topicos = plan.get("topicos", [])

    # --- FUN√á√ÉO DE VERIFICA√á√ÉO H√çBRIDA E DEFINITIVA ---
    def _is_company_match(plan_canonical_name: str, metadata_name: str) -> bool:
        if not plan_canonical_name or not metadata_name:
            return False
        
        # 1. Tentativa de correspond√™ncia precisa via cat√°logo (M√©todo Preferencial)
        chunk_canonical_name = company_lookup_map.get(metadata_name.lower())
        if chunk_canonical_name and chunk_canonical_name.lower() == plan_canonical_name.lower():
            return True
            
        # 2. Plano B: Se o cat√°logo falhou, usar normaliza√ß√£o flex√≠vel
        searchable_part = unicodedata.normalize('NFKD', plan_canonical_name.lower()).encode('ascii', 'ignore').decode('utf-8').split(' ')[0]
        
        return searchable_part in metadata_name.lower()

    def add_candidate(chunk_text, source_info):
        """Fun√ß√£o auxiliar para adicionar chunks √∫nicos √† lista de candidatos."""
        chunk_hash = hash(chunk_text)
        if chunk_hash not in candidate_chunks:
            source_info_clean = {
                "text": chunk_text,
                "company_name": source_info.get("company_name"),
                "doc_type": source_info.get("doc_type"),
                "source_url": source_info.get("source_url")
            }
            candidate_chunks[chunk_hash] = source_info_clean

    # --- IN√çCIO DO ROTEAMENTO DE ESTRAT√âGIA DE BUSCA ---
    
    canonical_name_from_plan = empresas[0] if empresas else None
    
    # ROTA 1: Plano especial para buscar o Item 8.4
    if plan_type == "section_8_4" and canonical_name_from_plan:
        search_name = next((entry.get("search_alias", canonical_name_from_plan) for entry in company_catalog_rich if entry.get("canonical_name") == canonical_name_from_plan), canonical_name_from_plan)
        logger.info(f"ROTA section_8_4: Usando nome de busca '{search_name}' para '{canonical_name_from_plan}'")

        search_query = f"detalhes sobre os t√≥picos {', '.join(topicos)} do item 8.4 da empresa {search_name}"
        artifacts_to_search = {'item_8_4': artifacts.get('item_8_4', {})}
        query_embedding = model.encode([search_query], normalize_embeddings=True).astype('float32')
        
        for doc_type, artifact_data in artifacts_to_search.items():
            if not artifact_data: continue
            index, chunks_map_data, all_chunks_data = (artifact_data.get(k) for k in ['index', 'chunks', 'chunks'])
            if not all([index, chunks_map_data, all_chunks_data]): continue
            chunks_map, all_chunks = chunks_map_data.get('map', []), all_chunks_data.get('chunks', [])
            
            if not all([chunks_map, all_chunks]): continue
            
            scores, indices = index.search(query_embedding, TOP_K_INITIAL_RETRIEVAL)
            for _, idx in enumerate(indices[0]):
                if idx != -1:
                    chunk_map_item = chunks_map[idx]
                    metadata_company_name = chunk_map_item.get('company_name', '')
                    
                    if _is_company_match(canonical_name_from_plan, metadata_company_name):
                        source_info = {"company_name": canonical_name_from_plan, "doc_type": doc_type, "source_url": chunk_map_item.get("source_url")}
                        add_candidate(all_chunks[idx], source_info)

    # ROTA 2: Planos Padr√£o (Default) - Cobre buscas gerais, h√≠bridas e resumos
    else:
        # CASO 2.1: Busca geral por t√≥pico (sem empresa)
        if not empresas and topicos:
            logger.info(f"ROTA Default (Geral): Executando busca conceitual para os t√≥picos: {topicos}")
            for topico in topicos:
                for term in expand_search_terms(topico, kb)[:3]:
                    search_query = f"explica√ß√£o detalhada sobre o conceito e funcionamento de {term}"
                    query_embedding = model.encode([search_query], normalize_embeddings=True).astype('float32')
                    for doc_type, artifact_data in artifacts.items():
                        if not artifact_data: continue
                        index = artifact_data.get('index')
                        chunks_map_data = artifact_data.get('chunks', {})
                        if not all([index, chunks_map_data]): continue

                        chunks_map = chunks_map_data.get('map', [])
                        all_chunks = chunks_map_data.get('chunks', [])
                        if not all([chunks_map, all_chunks]): continue
                        
                        scores, indices = index.search(query_embedding, TOP_K_SEARCH_FINAL)
                        for _, idx in enumerate(indices[0]):
                            if idx != -1:
                                chunk_map_item = chunks_map[idx]
                                source_info = {
                                    "company_name": chunk_map_item.get("company_name"),
                                    "doc_type": doc_type,
                                    "source_url": chunk_map_item.get("source_url")
                                }
                                add_candidate(all_chunks[idx], source_info)
        
        # CASO 2.2: Busca por empresa e t√≥picos (incluindo resumos)
        elif empresas and topicos:
            logger.info(f"ROTA Default (H√≠brida): Executando busca para Empresas: {empresas} e T√≥picos: {topicos}")
            for empresa_canonica in empresas:
                # Parte 1: Busca por Tags
                target_tags = set().union(*(expand_search_terms(t, kb) for t in topicos))
                tagged_chunks = search_by_tags(artifacts, empresa_canonica, list(target_tags))
                for chunk_info in tagged_chunks:
                     if _is_company_match(empresa_canonica, chunk_info.get("company_name", "")):
                        add_candidate(chunk_info['text'], chunk_info)
                
                # Parte 2: Busca Vetorial
                for topico in topicos:
                    search_name = next((entry.get("search_alias", empresa_canonica) for entry in company_catalog_rich if entry.get("canonical_name") == empresa_canonica), empresa_canonica)
                    search_query = f"informa√ß√µes detalhadas sobre {topico} no plano da empresa {search_name}"
                    query_embedding = model.encode([search_query], normalize_embeddings=True).astype('float32')

                    for doc_type, artifact_data in artifacts.items():
                        if not artifact_data: continue
                        index, chunks_map_data, all_chunks_data = (artifact_data.get(k) for k in ['index', 'chunks', 'chunks'])
                        if not all([index, chunks_map_data, all_chunks_data]): continue
                        chunks_map, all_chunks = chunks_map_data.get('map', []), all_chunks_data.get('chunks', [])
                        if not all([chunks_map, all_chunks]): continue

                        scores, indices = index.search(query_embedding, TOP_K_INITIAL_RETRIEVAL)
                        for _, idx in enumerate(indices[0]):
                            if idx != -1:
                                chunk_map_item = chunks_map[idx]
                                metadata_name = chunk_map_item.get('company_name', '')
                                
                                if _is_company_match(empresa_canonica, metadata_name):
                                    source_info = {"company_name": empresa_canonica, "doc_type": doc_type, "source_url": chunk_map_item.get("source_url")}
                                    add_candidate(all_chunks[idx], source_info)

    # --- ETAPA FINAL DE RE-RANKING (Comum a todas as rotas) ---
    if not candidate_chunks:
        logger.warning(f"Nenhum chunk candidato encontrado para a query: '{query}'")
        return "N√£o encontrei informa√ß√µes relevantes para esta consulta espec√≠fica.", []
    
    logger.info(f"Total de {len(candidate_chunks)} chunks candidatos √∫nicos encontrados. Re-ranqueando...")
    
    candidate_list = list(candidate_chunks.values())
    reranked_chunks = rerank_with_cross_encoder(query, candidate_list, cross_encoder_model, top_n=TOP_K_SEARCH_FINAL)
    
    full_context = ""
    retrieved_sources_structured = []
    seen_sources = set()

    for chunk in reranked_chunks:
        company_name = chunk.get('company_name', 'N/A')
        source_url = chunk.get('source_url', 'N/A')
        doc_type = chunk.get('doc_type', 'N/A')

        source_header = f"(Empresa: {company_name}, Documento: {doc_type})"
        clean_text = re.sub(r'\[(secao|topico):[^\]]+\]', '', chunk.get('text', '')).strip()
        full_context += f"--- CONTE√öDO RELEVANTE {source_header} ---\n{clean_text}\n\n"
        
        source_tuple = (company_name, source_url)
        if source_tuple not in seen_sources:
            seen_sources.add(source_tuple)
            retrieved_sources_structured.append(chunk)
            
    logger.info(f"Contexto final constru√≠do a partir de {len(reranked_chunks)} chunks re-ranqueados.")
    return full_context, retrieved_sources_structured
    
def create_dynamic_analysis_plan(query, company_catalog_rich, kb, summary_data):
    """
    Vers√£o 3.0 (Unificada) do planejador din√¢mico.

    Esta vers√£o combina o melhor de ambas as propostas:
    1.  EXTRAI filtros de metadados (setor, controle acion√°rio).
    2.  EXTRAI t√≥picos hier√°rquicos completos.
    3.  RESTAURA a detec√ß√£o de inten√ß√£o de "Resumo Geral" para perguntas abertas.
    4.  MANT√âM a detec√ß√£o da inten√ß√£o especial "Item 8.4".
    """
    logger.info(f"Gerando plano din√¢mico v3.0 para a pergunta: '{query}'")
    query_lower = query.lower().strip()
    
    plan = {
        "empresas": [],
        "topicos": [],
        "filtros": {},
        "plan_type": "default" # O tipo de plano default aciona a busca RAG padr√£o.
    }

    # --- PASSO 1: Extra√ß√£o de Filtros de Metadados (L√≥gica da Nova Proposta) ---
    FILTER_KEYWORDS = {
        "setor": ["bancos", "varejo", "energia", "saude", "metalurgia", "siderurgia"],
        "controle_acionario": ["privado", "privada", "estatal", "estatais", "p√∫blico", "p√∫blica"]
    }
    CANONICAL_MAP = {
        "privada": "Privado", "privados": "Privado", "privadas": "Privado",
        "estatal": "Estatal", "estatais": "Estatal", "p√∫blico": "Estatal", "p√∫blica": "Estatal"
    }
    
    for filter_type, keywords in FILTER_KEYWORDS.items():
        for keyword in keywords:
            if re.search(r'\b' + re.escape(keyword.lower()) + r'\b', query_lower):
                canonical_term = CANONICAL_MAP.get(keyword, keyword.capitalize())
                plan["filtros"][filter_type] = canonical_term
                logger.info(f"Filtro de '{filter_type}' encontrado: '{canonical_term}'")
                break

    # --- PASSO 2: Identifica√ß√£o Robusta de Empresas (L√≥gica Original Mantida) ---
    mentioned_companies = []
    if company_catalog_rich:
        companies_found_by_alias = {}
        for company_data in company_catalog_rich:
            canonical_name = company_data.get("canonical_name")
            if not canonical_name: continue
            
            all_aliases = company_data.get("aliases", []) + [canonical_name]
            for alias in all_aliases:
                if re.search(r'\b' + re.escape(alias.lower()) + r'\b', query_lower):
                    score = len(alias.split())
                    if canonical_name not in companies_found_by_alias or score > companies_found_by_alias[canonical_name]:
                        companies_found_by_alias[canonical_name] = score
        if companies_found_by_alias:
            mentioned_companies = [c for c, s in sorted(companies_found_by_alias.items(), key=lambda item: item[1], reverse=True)]

    if not mentioned_companies:
        for empresa_nome in summary_data.keys():
            if re.search(r'\b' + re.escape(empresa_nome.lower()) + r'\b', query_lower):
                mentioned_companies.append(empresa_nome)
    
    plan["empresas"] = mentioned_companies
    logger.info(f"Empresas identificadas: {plan['empresas']}")

    # --- PASSO 3: Detec√ß√£o de Inten√ß√µes Especiais (L√ìGICA UNIFICADA) ---
    # Palavras-chave para as inten√ß√µes especiais
    summary_keywords = ['resumo geral', 'plano completo', 'como funciona o plano', 'descreva o plano', 'resumo do plano', 'detalhes do plano']
    section_8_4_keywords = ['item 8.4', 'se√ß√£o 8.4', '8.4 do fre']
    
    is_summary_request = any(keyword in query_lower for keyword in summary_keywords)
    is_section_8_4_request = any(keyword in query_lower for keyword in section_8_4_keywords)

    if plan["empresas"] and is_section_8_4_request:
        plan["plan_type"] = "section_8_4"
        # O t√≥pico √© o caminho hier√°rquico para a se√ß√£o inteira
        plan["topicos"] = ["FormularioReferencia,Item_8_4"]
        logger.info("Plano especial 'section_8_4' detectado.")
        return {"status": "success", "plan": plan}
    
    # [L√ìGICA RESTAURADA E ADAPTADA]
    # Se for uma pergunta de resumo para uma empresa, define um conjunto de t√≥picos essenciais.
    elif plan["empresas"] and is_summary_request:
        plan["plan_type"] = "summary" # Um tipo especial para indicar um resumo completo
        logger.info("Plano especial 'summary' detectado. Montando plano com t√≥picos essenciais.")
        # Define os CAMINHOS HIER√ÅRQUICOS essenciais para um bom resumo.
        plan["topicos"] = [
            "TiposDePlano",
            "ParticipantesCondicoes,Elegibilidade",
            "Mecanicas,Vesting",
            "Mecanicas,Lockup",
            "IndicadoresPerformance",
            "GovernancaRisco,MalusClawback",
            "EventosFinanceiros,DividendosProventos"
        ]
        return {"status": "success", "plan": plan}

    # --- PASSO 4: Extra√ß√£o de T√≥picos Hier√°rquicos (Se Nenhuma Inten√ß√£o Especial Foi Ativada) ---
    alias_map = create_hierarchical_alias_map(kb)
    found_topics = set()
    
    # Ordena os aliases por comprimento para encontrar o mais espec√≠fico primeiro
    for alias in sorted(alias_map.keys(), key=len, reverse=True):
        # Usamos uma regex mais estrita para evitar matches parciais (ex: 'TSR' em 'TSR Relativo')
        if re.search(r'\b' + re.escape(alias) + r'\b', query_lower):
            found_topics.add(alias_map[alias])
    
    plan["topicos"] = sorted(list(found_topics))
    if plan["topicos"]:
        logger.info(f"Caminhos de t√≥picos identificados: {plan['topicos']}")

    # --- PASSO 5: Valida√ß√£o Final ---
    if not plan["empresas"] and not plan["topicos"] and not plan["filtros"]:
        logger.warning("Planejador n√£o conseguiu identificar empresa, t√≥pico ou filtro na pergunta.")
        return {"status": "error", "message": "N√£o foi poss√≠vel identificar uma inten√ß√£o clara na sua pergunta. Tente ser mais espec√≠fico."}
        
    return {"status": "success", "plan": plan}
    
def analyze_single_company(
    empresa: str, 
    plan: dict, 
    query: str,  # Novo argumento
    artifacts: dict, 
    model: SentenceTransformer, 
    cross_encoder_model: CrossEncoder, # Novo argumento
    kb: dict,
    execute_dynamic_plan_func: callable,
    get_final_unified_answer_func: callable
) -> dict:
    """
    Executa o plano de an√°lise para uma √∫nica empresa e retorna um dicion√°rio estruturado.
    Esta fun√ß√£o √© projetada para ser executada em um processo paralelo.
    """
    single_plan = {'empresas': [empresa], 'topicos': plan['topicos']}
    
    # --- CORRE√á√ÉO APLICADA AQUI ---
    # Adicionado o argumento 'is_summary_plan=False' na chamada.
    context, sources_list = execute_dynamic_plan_func(query, single_plan, artifacts, model, cross_encoder_model, kb)
    
    result_data = {
        "empresa": empresa,
        "resumos_por_topico": {topico: "Informa√ß√£o n√£o encontrada" for topico in plan['topicos']},
        "sources": sources_list
    }

    if context:
        summary_prompt = f"""
        Com base no CONTEXTO abaixo sobre a empresa {empresa}, crie um resumo para cada um dos T√ìPICOS solicitados.
        Sua resposta deve ser APENAS um objeto JSON v√°lido, sem nenhum texto adicional antes ou depois.
        
        T√ìPICOS PARA RESUMIR: {json.dumps(plan['topicos'])}
        
        CONTEXTO:
        {context}
        
        FORMATO OBRIGAT√ìRIO DA RESPOSTA (APENAS JSON):
        {{
          "resumos_por_topico": {{
            "T√≥pico 1": "Resumo conciso sobre o T√≥pico 1...",
            "T√≥pico 2": "Resumo conciso sobre o T√≥pico 2...",
            "...": "..."
          }}
        }}
        """
        
        try:
            json_response_str = get_final_unified_answer_func(summary_prompt, context)
            json_match = re.search(r'\{.*\}', json_response_str, re.DOTALL)
            if json_match:
                parsed_json = json.loads(json_match.group())
                result_data["resumos_por_topico"] = parsed_json.get("resumos_por_topico", result_data["resumos_por_topico"])
            else:
                logger.warning(f"N√£o foi poss√≠vel extrair JSON da resposta para a empresa {empresa}.")

        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Erro ao processar o resumo JSON para {empresa}: {e}")
            
    return result_data


def handle_rag_query(
    query: str, 
    artifacts: dict, 
    embedding_model: SentenceTransformer,  # <-- CORRE√á√ÉO APLICADA AQUI
    cross_encoder_model: CrossEncoder,
    kb: dict, 
    company_catalog_rich: list, 
    summary_data: dict
) -> tuple[str, list[dict]]:
    """
    Orquestra o pipeline de RAG para perguntas qualitativas, incluindo a gera√ß√£o do plano,
    a execu√ß√£o da busca (com re-ranking) e a s√≠ntese da resposta final.
    """
    with st.status("1Ô∏è‚É£ Gerando plano de an√°lise...", expanded=True) as status:
        plan_response = create_dynamic_analysis_plan(query, company_catalog_rich, kb, summary_data)
        
        if plan_response['status'] != "success":
            status.update(label="‚ö†Ô∏è Falha na identifica√ß√£o", state="error", expanded=True)
            
            st.warning("N√£o consegui identificar uma empresa conhecida na sua pergunta para realizar uma an√°lise profunda.")
            st.info("Para an√°lises detalhadas, por favor, use o nome de uma das empresas listadas na barra lateral.")
            
            with st.spinner("Estou pensando em uma pergunta alternativa que eu possa responder..."):
                alternative_query = suggest_alternative_query(query)
            
            st.markdown("#### Que tal tentar uma pergunta mais geral?")
            st.markdown("Voc√™ pode copiar a sugest√£o abaixo ou reformular sua pergunta original.")
            st.code(alternative_query, language=None)
            
            # Retornamos uma string vazia para o texto e para as fontes, encerrando a an√°lise de forma limpa.
            return "", []
        # --- FIM DO NOVO BLOCO ---
            
        plan = plan_response['plan']
        
        summary_keywords = ['resumo', 'geral', 'completo', 'vis√£o geral', 'como funciona o plano', 'detalhes do plano']
        is_summary_request = any(keyword in query.lower() for keyword in summary_keywords)
        
        specific_topics_in_query = list({canonical for alias, canonical in _create_flat_alias_map(kb).items() if re.search(r'\b' + re.escape(alias) + r'\b', query.lower())})
        is_summary_plan = is_summary_request and not specific_topics_in_query
        
        if plan['empresas']:
            st.write(f"**üè¢ Empresas identificadas:** {', '.join(plan['empresas'])}")
        else:
            st.write("**üè¢ Nenhuma empresa espec√≠fica identificada. Realizando busca geral.**")
            
        st.write(f"**üìù T√≥picos a analisar:** {', '.join(plan['topicos'])}")
        if is_summary_plan:
            st.info("üí° Modo de resumo geral ativado. A busca ser√° otimizada para os t√≥picos encontrados.")
            
        status.update(label="‚úÖ Plano gerado com sucesso!", state="complete")

    final_answer, all_sources_structured = "", []
    seen_sources_tuples = set()

    # --- L√≥gica para M√∫ltiplas Empresas (Compara√ß√£o) ---
    if len(plan.get('empresas', [])) > 1:
        st.info(f"Modo de compara√ß√£o ativado para {len(plan['empresas'])} empresas. Executando an√°lises em paralelo...")
        
        with st.spinner(f"Analisando {len(plan['empresas'])} empresas..."):
            with ThreadPoolExecutor(max_workers=len(plan['empresas'])) as executor:
                futures = [
                    executor.submit(
                        analyze_single_company, 
                        empresa, plan, query, artifacts, embedding_model, cross_encoder_model, kb,
                        execute_dynamic_plan, get_final_unified_answer
                    ) 
                    for empresa in plan['empresas']
                ]
                results = [future.result() for future in futures]

        for result in results:
            for src_dict in result.get('sources', []):
                company_name = src_dict.get('company_name')
                source_url = src_dict.get('source_url')
                
                if company_name and source_url:
                    src_tuple = (company_name, source_url)
                    if src_tuple not in seen_sources_tuples:
                        seen_sources_tuples.add(src_tuple)
                        all_sources_structured.append(src_dict)

        with st.status("Gerando relat√≥rio comparativo final...", expanded=True) as status:
            structured_context = json.dumps(results, indent=2, ensure_ascii=False)
            comparison_prompt = f"""
            Sua tarefa √© criar um relat√≥rio comparativo detalhado sobre "{query}".
            Use os dados estruturados fornecidos no CONTEXTO JSON abaixo.
            O relat√≥rio deve come√ßar com uma breve an√°lise textual e, em seguida, apresentar uma TABELA MARKDOWN clara e bem formatada.

            CONTEXTO (em formato JSON):
            {structured_context}
            """
            final_answer = get_final_unified_answer(comparison_prompt, structured_context)
            status.update(label="‚úÖ Relat√≥rio comparativo gerado!", state="complete")
            
    # --- L√≥gica para Empresa √önica ou Busca Geral ---
    else:
        with st.status("2Ô∏è‚É£ Recuperando e re-ranqueando contexto...", expanded=True) as status:
            context, all_sources_structured = execute_dynamic_plan(
                query, plan, artifacts, embedding_model, cross_encoder_model, kb)
            
            if not context:
                st.error("‚ùå N√£o encontrei informa√ß√µes relevantes nos documentos para a sua consulta.")
                return "Nenhuma informa√ß√£o relevante encontrada.", []
                
            st.write(f"**üìÑ Contexto recuperado de:** {len(all_sources_structured)} documento(s)")
            status.update(label="‚úÖ Contexto relevante selecionado!", state="complete")
        
        with st.status("3Ô∏è‚É£ Gerando resposta final...", expanded=True) as status:
            final_answer = get_final_unified_answer(query, context)
            status.update(label="‚úÖ An√°lise conclu√≠da!", state="complete")

    return final_answer, all_sources_structured

def main():
    st.title("ü§ñ Agente de An√°lise de Planos de Incentivo (ILP)")
    st.markdown("---")

    embedding_model, cross_encoder_model, artifacts, summary_data = setup_and_load_data()
        
    if not summary_data or not artifacts:
        st.error("‚ùå Falha cr√≠tica no carregamento dos dados. O app n√£o pode continuar.")
        st.stop()
    
    engine = AnalyticalEngine(summary_data, DICIONARIO_UNIFICADO_HIERARQUICO) 
    
    try:
        from catalog_data import company_catalog_rich 
    except ImportError:
        company_catalog_rich = [] 
    
    st.session_state.company_catalog_rich = company_catalog_rich

   
    from tools import _create_company_lookup_map
    st.session_state.company_lookup_map = _create_company_lookup_map(company_catalog_rich)


    with st.sidebar:
        st.header("üìä Informa√ß√µes do Sistema")
        st.metric("Categorias de Documentos (RAG)", len(artifacts))
        st.metric("Empresas no Resumo", len(summary_data))
        with st.expander("Empresas com dados no resumo"):
            st.dataframe(pd.DataFrame(sorted(list(summary_data.keys())), columns=["Empresa"]), use_container_width=True, hide_index=True)
        st.success("‚úÖ Sistema pronto para an√°lise")
        st.info(f"Embedding Model: `{MODEL_NAME}`")
        st.info(f"Generative Model: `{GEMINI_MODEL}`")
    
    st.header("üí¨ Fa√ßa sua pergunta")
    
    # Em app.py, localize o bloco `with st.expander(...)` e substitua seu conte√∫do por este:

    with st.expander("‚ÑπÔ∏è **Guia do Usu√°rio: Como Extrair o M√°ximo do Agente**", expanded=False): # `expanded=False` √© uma boa pr√°tica para n√£o poluir a tela inicial
        st.markdown("""
        Este agente foi projetado para atuar como um consultor especialista em Planos de Incentivo de Longo Prazo (ILP), analisando uma base de dados de documentos p√∫blicos da CVM. Para obter os melhores resultados, formule perguntas que explorem suas principais capacidades.
        """)

        st.subheader("1. Perguntas de Listagem (Quem tem?) üéØ")
        st.info("""
        Use estas perguntas para identificar e listar empresas que adotam uma pr√°tica espec√≠fica. Ideal para mapeamento de mercado.
        """)
        st.markdown("**Exemplos:**")
        st.code("""- Liste as empresas que pagam dividendos ou JCP durante o per√≠odo de car√™ncia (vesting).
        - Quais companhias possuem cl√°usulas de Malus ou Clawback?
        - Gere uma lista de empresas que oferecem planos com contrapartida do empregador (Matching/Coinvestimento).
        - Quais organiza√ß√µes mencionam explicitamente o Comit√™ de Remunera√ß√£o como √≥rg√£o aprovador dos planos?""")

        st.subheader("2. An√°lise Estat√≠stica (Qual a m√©dia?) üìà")
        st.info("""
        Pergunte por m√©dias, medianas e outros dados estat√≠sticos para entender os n√∫meros por tr√°s das pr√°ticas de mercado e fazer benchmarks.
        """)
        st.markdown("**Exemplos:**")
        st.code("""- Qual o per√≠odo m√©dio de vesting (em anos) entre as empresas analisadas?
        - Qual a dilui√ß√£o m√°xima m√©dia (% do capital social) que os planos costumam aprovar?
        - Apresente as estat√≠sticas do desconto no pre√ßo de exerc√≠cio (m√≠nimo, m√©dia, m√°ximo).
        - Qual o prazo de lock-up (restri√ß√£o de venda) mais comum ap√≥s o vesting das a√ß√µes?""")

        st.subheader("3. Padr√µes de Mercado (Como √© o normal?) üó∫Ô∏è")
        st.info("""
        Fa√ßa perguntas abertas para que o agente analise diversos planos e descreva os padr√µes e as abordagens mais comuns para um determinado t√≥pico.
        """)
        st.markdown("**Exemplos:**")
        st.code("""- Analise os modelos t√≠picos de planos de A√ß√µes Restritas (RSU), o tipo mais comum no mercado.
        - Al√©m do TSR, quais s√£o as metas de performance (ESG, Financeiras) mais utilizadas pelas empresas?
        - Descreva os padr√µes de tratamento para condi√ß√µes de sa√≠da (Good Leaver vs. Bad Leaver) nos planos.
        - Quais as abordagens mais comuns para o tratamento de dividendos em a√ß√µes ainda n√£o investidas?""")

        st.subheader("4. An√°lise Profunda e Comparativa (Me explique em detalhes) üß†")
        st.info("""
        Use o poder do RAG para pedir an√°lises detalhadas sobre uma ou mais empresas, comparando regras e estruturas espec√≠ficas.
        """)
        st.markdown("**Exemplos:**")
        st.code("""- Como o plano da Vale trata a acelera√ß√£o de vesting em caso de mudan√ßa de controle?
        - Compare as cl√°usulas de Malus/Clawback da Vale com as do Ita√∫.
        - Descreva em detalhes o plano de Op√ß√µes de Compra da Localiza, incluindo prazos, condi√ß√µes e forma de liquida√ß√£o.
        - Descreva o Item 8.4 da M.dias Braco.
        - Quais as diferen√ßas na elegibilidade de participantes entre os planos da Magazine Luiza e da Lojas Renner?""")


        st.subheader("‚ùó Conhecendo as Limita√ß√µes")
        st.warning("""
        - **Fonte dos Dados:** Minha an√°lise se baseia em documentos p√∫blicos da CVM com data de corte 31/07/2025. N√£o tenho acesso a informa√ß√µes em tempo real ou privadas.
        - **Identifica√ß√£o de Nomes:** Para an√°lises profundas, preciso que o nome da empresa seja claro e reconhec√≠vel. Se o nome for amb√≠guo ou n√£o estiver na minha base, posso n√£o encontrar os detalhes.
        - **Escopo:** Sou altamente especializado em Incentivos de Longo Prazo. Perguntas fora deste dom√≠nio podem n√£o ter respostas adequadas.
        """)

    user_query = st.text_area("Sua pergunta:", height=100, placeholder="Ex: Quais s√£o os modelos t√≠picos de vesting? ou Como funciona o plano da Vale?")
    
    if st.button("üîç Analisar", type="primary", use_container_width=True):
        if not user_query.strip():
            st.warning("‚ö†Ô∏è Por favor, digite uma pergunta.")
            st.stop()
        
        st.markdown("---")
        st.subheader("üìã Resultado da An√°lise")
        
        with st.spinner("Analisando a inten√ß√£o da sua pergunta..."):
            intent = get_query_intent_with_llm(user_query)

        if intent == "quantitativa":
            query_lower = user_query.lower()
            listing_keywords = ["quais empresas", "liste as empresas", "quais companhias"]
            thematic_keywords = ["modelos t√≠picos", "padr√µes comuns", "analise os planos", "formas mais comuns"]
            
            alias_map, _ = _create_alias_to_canonical_map(DICIONARIO_UNIFICADO_HIERARQUICO)
            topics_to_search = _get_all_canonical_topics_from_text(query_lower, alias_map)
            topics_to_search = [t for t in topics_to_search if t.lower() not in listing_keywords and t.lower() not in thematic_keywords]

            # Rota 1: An√°lise Tem√°tica
            if any(keyword in query_lower for keyword in thematic_keywords) and topics_to_search:
                primary_topic = topics_to_search[0]
                with st.spinner(f"Iniciando an√°lise tem√°tica... Este processo √© detalhado e pode levar alguns minutos."):
                    st.write(f"**T√≥pico identificado para an√°lise tem√°tica:** `{topics_to_search}`")
                    final_report = analyze_topic_thematically(
                        topic=topics_to_search, query=user_query, artifacts=artifacts, model=embedding_model, kb=DICIONARIO_UNIFICADO_HIERARQUICO,
                        execute_dynamic_plan_func=execute_dynamic_plan, get_final_unified_answer_func=get_final_unified_answer
                    )
                    st.markdown(final_report)

            # Rota 2: Listagem de Empresas
            elif any(keyword in query_lower for keyword in listing_keywords) and topics_to_search:
                with st.spinner(f"Usando ferramentas para encontrar empresas..."):
                    st.write(f"**T√≥picos identificados para busca:** `{', '.join(topics_to_search)}`")
        
                    all_found_companies = set()
        
                    # CORRE√á√ÉO: Itera sobre a lista e chama a ferramenta para CADA t√≥pico.
                    for topic_item in topics_to_search:
                        companies = find_companies_by_topic(
                            topic=topic_item,  # Passa um √∫nico t√≥pico (string)
                            artifacts=artifacts, 
                            model=embedding_model, 
                            kb=DICIONARIO_UNIFICADO_HIERARQUICO
                        )
                        all_found_companies.update(companies)

                    if all_found_companies:
                        sorted_companies = sorted(list(all_found_companies))
                        final_answer = f"#### Foram encontradas {len(sorted_companies)} empresas para os t√≥picos relacionados:\n"
                        final_answer += "\n".join([f"- {company}" for company in sorted_companies])
                    else:
                        final_answer = "Nenhuma empresa encontrada nos documentos para os t√≥picos identificados."
                
                st.markdown(final_answer)

            # Rota 3: Fallback para o AnalyticalEngine
            else:
                st.info("Inten√ß√£o quantitativa detectada. Usando o motor de an√°lise r√°pida...")
                with st.spinner("Executando an√°lise quantitativa r√°pida..."):
                    report_text, data_result = engine.answer_query(user_query)
                    if report_text: st.markdown(report_text)
                    if data_result is not None:
                        if isinstance(data_result, pd.DataFrame):
                            if not data_result.empty: st.dataframe(data_result, use_container_width=True, hide_index=True)
                        elif isinstance(data_result, dict):
                            for df_name, df_content in data_result.items():
                                if df_content is not None and not df_content.empty:
                                    st.markdown(f"#### {df_name}")
                                    st.dataframe(df_content, use_container_width=True, hide_index=True)
                    else: 
                        st.info("Nenhuma an√°lise tabular foi gerada para a sua pergunta ou dados insuficientes.")
        
        else: # intent == 'qualitativa'
            final_answer, sources = handle_rag_query(
                user_query, 
                artifacts, 
                embedding_model, 
                cross_encoder_model, 
                kb=DICIONARIO_UNIFICADO_HIERARQUICO,
                company_catalog_rich=st.session_state.company_catalog_rich, 
                summary_data=summary_data
            )
            st.markdown(final_answer)
            
            if sources:
                with st.expander(f"üìö Documentos consultados ({len(sources)})", expanded=True):
                    st.caption("Nota: Links diretos para a CVM podem falhar. Use a busca no portal com o protocolo como plano B.")
        
        # --- BLOCO CORRIGIDO ---
                    for src in sorted(sources, key=lambda x: x.get('company_name', '')):
                        company_name = src.get('company_name', 'N/A')
                        doc_type_raw = src.get('doc_type', '')
                        url = src.get('source_url', '')

                        if doc_type_raw == 'outros_documentos':
                            display_doc_type = 'Plano de Remunera√ß√£o'
                        else:
                            display_doc_type = doc_type_raw.replace('_', ' ')
            
                        display_text = f"{company_name} - {display_doc_type}"
            
                        # A l√≥gica de exibi√ß√£o agora est√° corretamente separada por tipo de documento
                        if "frmExibirArquivoIPEExterno" in url:
                            # O protocolo S√ì √© definido e usado dentro deste bloco
                            protocolo_match = re.search(r'NumeroProtocoloEntrega=(\d+)', url)
                            protocolo = protocolo_match.group(1) if protocolo_match else "N/A"
                            st.markdown(f"**{display_text}** (Protocolo: **{protocolo}**)")
                            st.markdown(f"‚Ü≥ [Link Direto para Plano de ILP]({url}) ", unsafe_allow_html=True)
            
                        elif "frmExibirArquivoFRE" in url:
                            # Este bloco n√£o usa a vari√°vel 'protocolo'
                            st.markdown(f"**{display_text}**")
                            st.markdown(f"‚Ü≥ [Link Direto para Formul√°rio de Refer√™ncia]({url})", unsafe_allow_html=True)
            
                        else:
                            # Este bloco tamb√©m n√£o usa a vari√°vel 'protocolo'
                            st.markdown(f"**{display_text}**: [Link]({url})")


if __name__ == "__main__":
    main()
